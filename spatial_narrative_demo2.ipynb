{"cells":[{"cell_type":"markdown","metadata":{"id":"aUojVOiIhoEv"},"source":["# **Extracting Spatial Entities from text (2)**\n","---"]},{"cell_type":"markdown","metadata":{"id":"iZ0kCbYUh4gu"},"source":["## Task Description:"]},{"cell_type":"markdown","metadata":{"id":"fcfL-sEdGCIm"},"source":["![](https://raw.githubusercontent.com/IgnatiusEzeani/spatial_narratives_workshop/main/img/from_penrith_tagged.png)\n","\n","Assuming we know nothing about the geography of the place(s) described by the corpus, what can we learn about it. In particular:\n","* **What places are there?** These can be:\n"," * `Toponyms` (*Keswick*, *Pooley Bridge*, *the River Lowther*)\n"," * `Geographical features` (*the town*, *a hill*, *the road*)\n"," * `Locative adverbs` (*above*, *north-of*, *eastwards*, *here*, *there*)\n"," "]},{"cell_type":"markdown","source":["## Named Entity Recognition and Semantic Tagging\n","Previously, we applied the a rule based method to spatial elements extraction from text in **Demo 1**.\n","\n","There were a few limitations with the rule-based method.\n","* It requires a complete list of entities.\n","* Rules need to be provided for all possible scenarios:\n","  - e.g. spelling errors, variations in capitalizations, inflections etc.\n","  - Over-lapping instances ('Eamont' vs 'Eamont Bridge')\n","* Difficult to extract references to time and date as well as sentiments and emotions.\n","* Does not generalize well with other corpora\n","\n","In this demo, we will explore the option of adapting named entity recognition and semantic tagging systems. "],"metadata":{"id":"lZQex_MqCF6Q"}},{"cell_type":"markdown","source":["## **Step 1: Downloading the workshop materials**\n","Let's download (clone) the resources for the workshop from the [Spatial Narrative Demo](https://github.com/SpaceTimeNarratives/demo)  GitHub repository."],"metadata":{"id":"VQzZg_4jQ9Ch"}},{"cell_type":"code","source":["!git clone https://github.com/SpaceTimeNarratives/demo.git"],"metadata":{"id":"16hV3t2_K-Uj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://raw.githubusercontent.com/SpaceTimeNarratives/demo/main/img/file_structure.png\" width=\"300\"/>\n","</div>\n","\n","The `demo` directory contains an example file `example_text.txt`. Our aim is to read file and display the text as well as identify all the place names mentioned in the text.\n","\n","### Changing into the working directory\n","Everything we need for this exercise can be found in the working directory (or folder) named *demo*. We will use the `os` (operating system) library which contains all the useful functions we may need to manage our folders and files programmatically. \n","\n","Here we use the `chdir()` (change directory) function to get into our working directory and list the contents of our directory using the `listdir()` \n","\n","Run the code below to change to the working directory `demo/` and list its content."],"metadata":{"id":"-gHbsXiiSIGX"}},{"cell_type":"code","source":["# Type or paste the command below:\n","import os\n","os.chdir('demo/')\n","os.listdir()"],"metadata":{"id":"cSzKcVhYR-1r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 2: Installing and importing `spaCy`**\n","The [spaCy NLP](https://spacy.io/) library provides a named entity recognizer that we can use."],"metadata":{"id":"8wf3pdzkWFS3"}},{"cell_type":"markdown","source":["### Installing `spacy v3.3.1`\n","The current version of `PyMUSAS` is compatible with `spacy v3.3.1`. However, Google Colab ships with `spacy v3.4.4` and its dependencies by default.\n","\n","So we will take the following steps to install the compatible version:\n","- uninstall the current version with `pip uninstall -y spacy`\n","- install `spacy v3.3.1` and other dependencies from the `requirements.txt` file in the working folder.\n","- run the `function.py` files which will import `spaCy` as well as all the other required libraries."],"metadata":{"id":"Gj1Du3Pvy8Ga"}},{"cell_type":"code","source":["pip -q uninstall -y spacy"],"metadata":{"id":"zFIGmgw-y4W2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip -q install -r requirements.txt"],"metadata":{"id":"vcF6smr3wuIX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Running the `function.py` file \n","We have wrapped up all the functions we created in the previous demo into a Python file `functions.py`. \n","\n","One of the functions depend on the `lemminflect` Python library that helps to generate lemmas and inflections, so we have to install it first."],"metadata":{"id":"-KrCvbY9FZ_G"}},{"cell_type":"code","source":["%run functions.py"],"metadata":{"id":"IImMWCj9FA_K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Importing `spaCy`\n","We need to import the `spaCy` NLP pipeline\n","and load the small version of the English model `en_core_web_sm` for tokenization, tagging, parsing and named entity recognition."],"metadata":{"id":"lkJtHFM5OIsn"}},{"cell_type":"code","source":["import spacy"],"metadata":{"id":"07HSvAlIHkD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"KGD-O_OHIsfy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 3: Extracting named entitities**\n"],"metadata":{"id":"xN-1wXUVx3my"}},{"cell_type":"markdown","source":["### Process `example.txt`\n","Read the content of the `example.txt` file in the working folder into the `example_text` variable and pass the variable through the NLP pipeline to produce the `spacy_processed` document."],"metadata":{"id":"KKTwhz36JauW"}},{"cell_type":"code","source":["example_text = open('example.txt').read()\n","example_text"],"metadata":{"id":"vsZtSxJNJlM7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_processed = nlp(example_text)"],"metadata":{"id":"Xk2uum74MHWd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's see all the entities and tags the tagger has seen from `processed_text`..."],"metadata":{"id":"WLXOuT5qM_Yi"}},{"cell_type":"code","source":["for entity in spacy_processed.ents:\n","    print(entity.text, entity.label_)"],"metadata":{"id":"fzqp4BQiNKyd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This looks quite interesting. We have more tags in the spacy NER tagger and, unsurprisingly, there is no `PLNAME` or `GEONOUN`. We can see that even without a list, it identified similar entities to the regular expression method, and even more (e.g. `King Arthur's Round Table` and `Lowther Castle`), although with different tags."],"metadata":{"id":"CdnInQGvNyB9"}},{"cell_type":"markdown","source":["### Visualizing the entities\n","Spacy has an inbuilt visualisation feature which we can call as below:\n","\n","```python\n","from spacy import displacy\n","HTML(displacy.render(spacy_processed, style=\"ent\"))\n","```\n","However, to have a bit of control over the visualization, we can use the `visualizer()` we built in the last demo. Therefore we need a function `extract_spacy_entities(spacy_processed)` to extract the spacy entities into a dictionary and build the list of tagged tokens for visualization.\n","\n","We can convert all tags refering to a place (e.g. `GPE`, `ORG` and `FAC`) to `PLNAME`. Also we have redefined the `BG_COLOR` variable in the `function.py` file to accommodate all possible spacy NER tags (See pages 21 & 22 of the [OntoNotes 5.0](https://catalog.ldc.upenn.edu/docs/LDC2013T19/OntoNotes-Release-5.0.pdf) document for the descriptions of the NER tags) and others that we may need later.\n","\n","```python\n","BG_COLOR = {\n","    'PLNAME':'#feca74','GEONOUN': '#9cc9cc', 'GPE':'#feca74', 'CARDINAL':'#e4e7d2',\n","    'FAC':'#9cc9cc','QUANTITY':'#e4e7d2','PERSON':'#aa9cfc', 'ORDINAL':'#e4e7d2',\n","    'ORG':'#7aecec', 'NORP':'#d9fe74', 'LOC':'#9ac9f5', 'DATE':'#c7f5a9',\n","    'PRODUCT':'#edf5a9', 'EVENT': '#e1a9f5','TIME':'#a9f5bc', 'WORK_OF_ART':'#e6c1d7',\n","    'LAW':'#e6e6c1','LANGUAGE':'#c9bdc7', 'PERCENT':'#c9ebf5', 'MONEY':'#b3d6f2',\n","    'EMOTION':'#f2ecd0', 'TIME-sem':'#d0e0f2', 'MOVEMENT':'#f2d0d0','no_tag':'#FFFFFF'\n","}\n","```"],"metadata":{"id":"PL3ySp9PQkfF"}},{"cell_type":"code","source":["def extract_spacy_entities(processed_text):\n","  entities = {}\n","  for ent in processed_text.ents:\n","    tag='PLNAME' if ent.label_ in ['GPE', 'ORG', 'FAC', 'LOC'] else ent.label_\n","    entities[ent.start_char] = ent.text, tag\n","  return OrderedDict(sorted(entities.items()))\n","\n","spacy_entities = extract_spacy_entities(spacy_processed)\n","visualize(example_text, spacy_entities)"],"metadata":{"id":"voXEM2hssahQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Combining the rule-based and `spaCy` methods\n","As shown above, some of the placenames are tagged as `PERSON`. However, the regular expression rules method used previously was able to identify them as placenames using the Lake District gazetteer.\n","\n","So using the `merge_entities()` function, we can combine the outputs from both methods in a way that the output of the regular expression overrides that of spacy NER where there is conflict."],"metadata":{"id":"8mh42MbbperQ"}},{"cell_type":"code","source":["# Read LD placenames into a list\n","ld_place_names = [name.strip() for name in open('LD_placenames.txt').readlines()]\n","\n","regex_entities = extract_entities(example_text, ld_place_names)"],"metadata":{"id":"3ZLKHuqUmhDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize(example_text, merge_entities(regex_entities, spacy_entities))"],"metadata":{"id":"Ol_wIppZnEpI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Including Geo nouns and locative adverbs\n","Great so far! We can also include the geo nouns and the locative adverbs in our extracted entity types using their respective lists.\n","\n","Remember that `merge_entities()` accepts two extracted entities arguments and the order matters because the first overrides the other when there is a conflict."],"metadata":{"id":"ORrOprrIwS_c"}},{"cell_type":"code","source":["# Extract geo nouns...\n","geonouns = get_inflections([noun.strip() for noun in open('geo_feature_nouns.txt').readlines()])\n","extracted_geonouns = extract_entities(example_text, geonouns, tag='GEONOUN')\n","\n","# Extract locative adverbs \n","loc_advs = [adv.split()[0] for adv in open('locativeAdverbs.txt').readlines()]\n","extracted_locadvs = extract_entities(example_text, geonouns,  tag='LOCADV')\n","\n","# Merger the entities in the other below\n","merged_entities = merge_entities(regex_entities,\n","                    merge_entities(spacy_entities,\n","                       merge_entities(extracted_geonouns, extracted_locadvs)))\n","# Visualize the merged entities.\n","visualize(example_text, merged_entities)"],"metadata":{"id":"s7WnB0wunEmR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### So far...\n","...this is going great 😊. \n","\n","##### **Rule based method**\n","With the rule based method, we can identify, extract and merge any number of entity types (placenames, geo nouns, locative adverbs) as long as we have a list of named elements in that class. But it is inefficient to create an exhaustive list that can generalize across different writings. \n","\n","##### **Named entity recognizer**\n","Fortunately, we can use a named entity recognizer identify interesting entities even without a list of elements. Although the NER model was not trained for our case study, we can adapt the relevant tags from the model output that correspond to our categories e.g. converting `[GPE, ORG, FAC, LOC]` to `PLNAME`."],"metadata":{"id":"E4FaSHapghyo"}},{"cell_type":"markdown","source":["## **Step 4: Extracting semantic entitities**\n","\n","We may also want extract spatial elements that indicate the movements, emotion or a sense of time. The previous methods are not capable of identifying references to movements and emotions.\n","\n","Altough the NER method can detect time and date references (e.g. `August`, `daily`, `the year 1635`), it is not able to pick expressions like `retrospective`, `for some time`, `never ending`, `prolonged` etc., as references to time.\n","\n","We will use **P**ython **M**ultilingual **U**crel **S**emantic **A**nalysis **S**ystem ([PyMUSAS](https://ucrel.github.io/pymusas/)), which is a rule based token and Multi Word Expression semantic tagger that uses the [Ucrel Semantic Analysis System (USAS)](https://ucrel.lancs.ac.uk/usas/) and runs on the spaCy pipeline.\n","\n","The USAS tagset three highlevel tags:\n","* `E` - Emotion \n","* `M` - Movement, location, travel and transport\n","* `T` - Time\n"],"metadata":{"id":"RJ0fCTEeav5O"}},{"cell_type":"markdown","source":["### Setting up `PyMUSAS`\n","\n"],"metadata":{"id":"aS56gcNhwP0m"}},{"cell_type":"code","source":["# Load the English PyMUSAS rule based tagger in a separate spaCy pipeline\n","english_tagger_pipeline = spacy.load('en_dual_none_contextual')\n","# Adds the English PyMUSAS rule based tagger to the main spaCy pipeline\n","nlp.add_pipe('pymusas_rule_based_tagger', source=english_tagger_pipeline)"],"metadata":{"id":"ei973adprKnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spacy_processed = nlp(example_text)"],"metadata":{"id":"uujMTAS3rZr5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tag_types = ['EMOTION', 'MOVEMENT', 'TIME-SEM']\n","semtag_entities = extract_sem_entities(spacy_processed, tag_types)\n","visualize(example_text, semtag_entities)"],"metadata":{"id":"JyP5EAhznEgI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_entities = merge_entities(merged_entities, semtag_entities)"],"metadata":{"id":"LOvUadekAme2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize(example_text, merged_entities)"],"metadata":{"id":"1A1AHtQ0D8T4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Putting it all together..**"],"metadata":{"id":"WifH5PM1EddJ"}},{"cell_type":"markdown","source":["Below is the functions that support all the activities in this demo as defined in the `functions.py` file.\n","\n","```python\n","\"\"\"\n","This code provides functions to extract and visualize entities and semantic tokens from text. \n","Here are the descriptions of the functions:\n","\"\"\"\n","\n","import re\n","from IPython.display import HTML\n","from collections import OrderedDict\n","from lemminflect import getLemma, getInflection\n","\n","BG_COLOR = {\n","    'PLNAME':'#feca74','GEONOUN': '#9cc9cc', 'GPE':'#feca74', 'CARDINAL':'#e4e7d2',\n","    'FAC':'#9cc9cc','QUANTITY':'#e4e7d2','PERSON':'#aa9cfc', 'ORDINAL':'#e4e7d2',\n","    'ORG':'#7aecec', 'NORP':'#d9fe74', 'LOC':'#9ac9f5', 'DATE':'#c7f5a9',\n","    'PRODUCT':'#edf5a9', 'EVENT': '#e1a9f5','TIME':'#a9f5bc', 'WORK_OF_ART':'#e6c1d7',\n","    'LAW':'#e6e6c1','LANGUAGE':'#c9bdc7', 'PERCENT':'#c9ebf5', 'MONEY':'#b3d6f2',\n","    'EMOTION':'#f2ecd0', 'TIME-SEM':'#d0e0f2', 'MOVEMENT':'#f2d0d0','no_tag':'#FFFFFF'\n","}\n","\n","\"\"\"\n","Function `extract_entities(text, ent_list, tag='PLNAME')`\n","This function takes a text, a list of entities (as strings), and an optional tag as input, \n","and returns a dictionary of entities with their indexes in the text as keys. \n","The optional tag parameter is used to specify the entity type, which defaults to `'PLNAME'` if not provided.\n","\"\"\"\n","def extract_entities(text, ent_list, tag='PLNAME'):\n","  sorted(set(ent_list), key=lambda x:len(x), reverse=True)\n","  extracted_entities = {}\n","  for ent in ent_list:\n","    for match in re.finditer(f' {ent}[\\.,\\s\\n;:]', text):\n","      # modified to return the `tag` too...\n","      extracted_entities[match.start()+1]=text[match.start()+1:match.end()-1], tag\n","  return {i:extracted_entities[i] for i in sorted(extracted_entities.keys())}\n","\n","combine = lambda x, y: (x[0], x[1], x[2]+' '+y[2], x[3])\n","\n","\"\"\"\n","Function `get_inflections(names_list)`\n","This function takes a list of geo nouns as input and returns a list of their inflections and lemmas using the `lemminflect` package.\n","\"\"\"\n","def get_inflections(names_list):\n","    gf_names_inflected = []\n","    for w in names_list:\n","      gf_names_inflected.append(w)\n","      gf_names_inflected.extend(list(getInflection(w.strip(), tag='NNS', inflect_oov=False)))\n","      gf_names_inflected.extend(list(getLemma(w.strip(), 'NOUN', lemmatize_oov=False)))\n","    return list(set(gf_names_inflected))\n","\n","\"\"\"\n","Function `combine_multi_tokens(a_list)`\n","This function takes a list of adjacent semantic tokens (a semantic token is a tuple of a token\n","and its tag) as input and returns a list of tuples where adjacent tokens of the same type are \n","combined into a single tuple.\n","Example: `[('at','TIME'), ('this','TIME'), ('point','TIME')] => [('at this point', 'TIME')]` \n","\"\"\"\n","def combine_multi_tokens(a_list):\n","  new_list = [a_list.pop()]\n","  while a_list:\n","    last = a_list.pop()\n","    if new_list[-1][0] - last[0] == 1:\n","      new_list.append(combine(last, new_list.pop()))\n","    else:\n","      new_list.append(last)\n","  return sorted(new_list)\n","\n","\"\"\"\n","Function `extract_sem_entities(processed_text, tag_types)`\n","This function takes processed text and a list of semantic tags as input and returns a\n","dictionary of semantic entities with their indexes in the text as keys.\n","\"\"\"\n","def extract_sem_entities(processed_text, tag_types):\n","  entities, tokens = {}, [token.text for token in processed_text]\n","  for tag_type in tag_types:\n","    tag_indices = [(i, token.idx, token.text, tag_type) for i, token in enumerate(processed_text) \n","                        if token._.pymusas_tags[0].startswith(tag_type[0])]\n","    if tag_indices:\n","      for i, idx, token, tag in combine_multi_tokens(tag_indices):\n","        entities[idx] = token, tag\n","  return OrderedDict(sorted(entities.items()))\n","\n","\"\"\"\n","Function `merge_entities(first_ents, second_ents)`\n","This function takes two dictionaries of entities and returns a merged dictionary.\n","\"\"\"\n","def merge_entities(first_ents, second_ents):\n","  return OrderedDict(sorted({** second_ents, **first_ents}.items()))\n","\n","\"\"\"\n","Function `get_tagged_list(text, entities)`\n","This function takes text and a dictionary of entities as input and returns a list of tuples where\n","each tuple contains a token and its tag.\n","\"\"\"\n","def get_tagged_list(text, entities):\n","  begin, tokens_tags = 0, []\n","  for start, (ent, tag) in entities.items():\n","    if begin <= start:\n","      tokens_tags.append((text[begin:start], None))\n","      tokens_tags.append((text[start:start+len(ent)], tag))\n","      begin = start+len(ent)\n","  tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n","  return tokens_tags\n","\n","\"\"\"\n","Function `mark_up(token, tag=None)`\n","This function takes a token and an optional tag as input and returns the token surrounded\n","by HTML markup that will be used for visualization. If a tag is provided, the token will be \n","highlighted with a background color corresponding to the tag.\n","\"\"\"\n","def mark_up(token, tag=None):\n","  if tag:\n","    begin_bkgr = f'<bgr class=\"entity\" style=\"background: {BG_COLOR[tag]}; padding: 0.05em 0.05em; margin: 0 0.15em;  border-radius: 0.55em;\">'\n","    end_bkgr = '\\n</bgr>'\n","    begin_span = '<span style=\"font-size: 0.8em; font-weight: bold; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">'\n","    end_span = '\\n</span>'\n","    return f\"{begin_bkgr}{token}{begin_span}{tag}{end_span}{end_bkgr}\"\n","  return f\"{token}\"\n","\n","\"\"\"\n","Function `visualize(text, entities)`\n","This function takes text and a dictionary of entities as input and returns an HTML-formatted\n","string that visually highlights the entities in the text. \n","\"\"\"\n","def visualize(text, entities):\n","  token_tag_list = get_tagged_list(text, entities)\n","  start_div = f'<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">'\n","  end_div = '\\n</div>'\n","  html = start_div\n","  for token, tag in token_tag_list:\n","    html += mark_up(token,tag)\n","  html += end_div\n","  return HTML(html)\n","\n","```"],"metadata":{"id":"sgSSQ8ZxEvoT"}},{"cell_type":"markdown","source":["## **Next step...**\n"],"metadata":{"id":"cXWNJZ4_l_pq"}},{"cell_type":"markdown","source":["**Using other text files**\n","\n","So far we have been able to extract placenames, geo nouns, locative adverbs as well as semantic depictions of the concepts of emotion, movement and time. \n","\n","In the next notebook, we will put these techniques together to build a demo that allows us to use our own files and choose the options we are interested in."],"metadata":{"id":"JFuaIOmSmBCa"}}],"metadata":{"colab":{"collapsed_sections":["VQzZg_4jQ9Ch","LCQBR8lJTAii","jOdjjJwFHv11","vb8f7aoPsYgL","Zw9jVk97R4SB","1tCywxkPi1r2","WwolMngoq_hs"],"provenance":[{"file_id":"https://github.com/SpaceTimeNarratives/demo/blob/main/spatial_narrative_demo2.ipynb","timestamp":1677263770373}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}