{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hawLYJA4jFhL",
        "ttNoFBaro9J8",
        "VJDnhDTjsF7K"
      ],
      "mount_file_id": "1cwMtOAqd69o5kT7mC35S4zF4ycFxRNsm",
      "authorship_tag": "ABX9TyOM205U5eHw3u9sy+nyUpB+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnatiusEzeani/spatial_narratives_workshop/blob/main/spatial_narrative_demo4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a dataset from corpus"
      ],
      "metadata": {
        "id": "PgfZqodKe0Ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Corpus files\n",
        "\n",
        "These are sample .xml files from the [Corpus of the Lake District Writing](https://www.lancaster.ac.uk/fass/projects/spatialhum.wordpress/?page_id=64#:~:text=The%20Corpus%20of%20Lake%20District,Poly%2DOlbion%20(1622).). They are annotated with important features for a previous project which could be relevant for us in future. However, we want to build a method that works on plain text too.\n",
        "\n",
        "Execute the code below to change into the `corpus` folder and view the list of files in the `files` folder:"
      ],
      "metadata": {
        "id": "hawLYJA4jFhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/UCREL/demo/work_in_progress/corpus/"
      ],
      "metadata": {
        "id": "dOjFw7a9kUjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also see what the content of the files look like by looking at one of them `Anon_cqp_66.xml`"
      ],
      "metadata": {
        "id": "HOS76i1VkoLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head files/Anon_cqp_66.xml"
      ],
      "metadata": {
        "id": "wjP8mEZjk9q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write a function `clean_text()` to remove the tags and return 'cleaned' text."
      ],
      "metadata": {
        "id": "i4N3gyelmuUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import re"
      ],
      "metadata": {
        "id": "hukEVDOrneqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleans the text of tags and punctuations. Assumes .xml file\n",
        "def clean_text(input_text):\n",
        "  soup = BeautifulSoup(input_text, 'xml')\n",
        "  # Define a regular expression pattern to match XML tags\n",
        "  pattern = re.compile(r'<.*?>')\n",
        "  _text = re.sub(pattern, '', input_text)\n",
        "\n",
        "  # use the nltk sentence tokenizer to segment the text into sentences\n",
        "  _text = _text.replace('\\n', ' ').replace('\\t', ' ').replace('âˆ«', 's')\n",
        "  _text = re.sub(r'\\s+', ' ', _text)\n",
        "  return _text.strip()"
      ],
      "metadata": {
        "id": "ej_FtMh4nD5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's open and read the file `Anon_cqp_66.xml` and pass it to the `clean_text()` function"
      ],
      "metadata": {
        "id": "fzJY0RNBoFXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Anon_cqp_66.xml'\n",
        "clean_text(open(f'files/{filename}').read())"
      ],
      "metadata": {
        "id": "fDiv2hk2nncy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the corpus file\n",
        "\n",
        "Here we want present the content of the file in a data table. Each row will represent a sentence while the columns will be the relevant details e.g. `sent_id`, `text`, `place names`(and the positions in text). `geo feature nouns`, `locative adverbs`, `spatial prepositions` etc.\n",
        "\n",
        "We can also include a column for the `sentiment score` on the text.\n"
      ],
      "metadata": {
        "id": "ttNoFBaro9J8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries..."
      ],
      "metadata": {
        "id": "z74g8_5x4uXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install -r resources/requirements.txt"
      ],
      "metadata": {
        "id": "z-ZdbJ5-1N5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's pass all the corpus files through the `clean_text()` function and store the outputs for each file in a dictionary `clean_texts`"
      ],
      "metadata": {
        "id": "j-gcm8YY8aIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "cleaned_texts = {f: clean_text(open(f\"files/{f}\").read())\n",
        "                for f in os.listdir('files/') if f.endswith('.xml')}"
      ],
      "metadata": {
        "id": "8trXTwGr7dQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the function file for the required functions"
      ],
      "metadata": {
        "id": "ikCbSEaO4_z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run resources/functions.py"
      ],
      "metadata": {
        "id": "KT8iRthl01No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all the lists for the entity categories"
      ],
      "metadata": {
        "id": "BOdWhZwN5Qjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of placenames and geonouns\n",
        "place_names = [name.strip().title().replace(\"'S\", \"'s\") for name in open('resources/LD_placenames.txt').readlines()] #read and convert to title case\n",
        "place_names += [name.upper() for name in place_names] #retain the upper case versions\n",
        "geonouns = get_inflections([noun.strip() for noun in open('resources/geo_feature_nouns.txt').readlines()])\n",
        "\n",
        "# Get the locative adverbs\n",
        "loc_advs = [l.split()[0] for l in open('resources/locative_adverbs.txt').readlines()]\n",
        "sp_prep  = [l.strip() for l in open('resources/spatial_prepositions.txt').readlines()\n",
        "                                                            if len(l.strip())>2]\n",
        "# Get distances\n",
        "distances = [l.strip() for l in open('resources/distances.txt').readlines()]\n",
        "\n",
        "# Get dates\n",
        "dates     = [l.strip() for l in open('resources/dates.txt').readlines()]\n",
        "\n",
        "# Get times\n",
        "times     = [l.strip() for l in open('resources/times.txt').readlines()]\n",
        "\n",
        "# Get events\n",
        "events    = [l.strip() for l in open('resources/events.txt').readlines()]\n",
        "\n",
        "# Get the list of positive and negative words from the sentiment lexicon\n",
        "pos_words = [w.strip() for w in open('resources/positive-words.txt','r', encoding='latin-1').readlines()[35:]]\n",
        "neg_words = [w.strip() for w in open('resources/negative-words.txt','r', encoding='latin-1').readlines()[35:]]"
      ],
      "metadata": {
        "id": "KnQJf3p7sBP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the `Spacy`'s small English model and modify the `ner` rules by adding our patterns to pipeline with `Entity Ruler`"
      ],
      "metadata": {
        "id": "nTWcrRJd5xBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model for extracting spatial entities"
      ],
      "metadata": {
        "id": "VJDnhDTjsF7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build the `Spacy`'s `ner EntityRuler`. We need to install the required libraries from the `requirements.txt`, then load and process the necessary files from the `resources` folder."
      ],
      "metadata": {
        "id": "kPJGY1nMuNmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternatively, load the small spacy English model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Add the `entity_ruler` to the pipeline before the NER module\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
        "\n",
        "\n",
        "# Define the patterns for the EntityRuler by labelling all the names with the tag PLNAME\n",
        "patterns =  [{\"label\": \"PLNAME\",  \"pattern\": plname} for plname in set(place_names)]\n",
        "patterns += [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in geonouns]\n",
        "patterns += [{\"label\": \"+EMOTION\", \"pattern\": word} for word in pos_words]\n",
        "patterns += [{\"label\": \"-EMOTION\", \"pattern\": word} for word in neg_words]\n",
        "patterns += [{\"label\": \"EVENT\",   \"pattern\": word} for word in events]\n",
        "patterns += [{\"label\": \"DATE\", \"pattern\": word} for word in dates]\n",
        "patterns += [{\"label\": \"TIME\", \"pattern\": word} for word in times]\n",
        "patterns += [{\"label\": \"DISTANCE\", \"pattern\": word} for word in distances]\n",
        "patterns += [{\"label\": \"LOCADV\", \"pattern\": word} for word in loc_advs]\n",
        "patterns += [{\"label\": \"SP-PREP\", \"pattern\": word} for word in sp_prep]\n",
        "\n",
        "ruler.add_patterns(patterns)"
      ],
      "metadata": {
        "id": "v1wzBBzPuxoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a tag name for each entity category"
      ],
      "metadata": {
        "id": "MNaxJw3f9tbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "header_tag = [('plnames', 'PLNAME'), ('geonouns', 'GEONOUN'), ('pos_words', '+EMOTION'),\n",
        "              ('neg_words', '-EMOTION'), ('events', 'EVENT'), ('dates', 'DATE'),\n",
        "              ('times', \t'TIME'), ('distances', 'DISTANCE'), ('loc_advs', 'LOCADV'),\n",
        "              ('spa_preps', 'SP-PREP')]\n",
        "\n",
        "# keep all the tags here...\n",
        "tags = [tag for _, tag in header_tag]\n",
        "\n",
        "# label the entity span with the right tag\n",
        "tagger = lambda d, t: [(ent,ent.start_char, ent.end_char) for ent in d.ents if ent.label_==t]"
      ],
      "metadata": {
        "id": "YLjUUKqq9F5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import `nltk` and `pandas`, then define the `generate_sent_dataset()` function. `tqdm` will help us to monitor the progress of the process.\n",
        "\n",
        "Also, we need the `pre_process_text()` function to lemmatize words, remove stopwords and punctuations while computing the sentiments."
      ],
      "metadata": {
        "id": "FF955g0Y_Vyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = stopwords.words('english')\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "zCQE6X1H_zTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_text(text):\n",
        "  return list(filter(lambda token: token not in string.punctuation,\n",
        "             [lemma.lemmatize(word) for word in word_tokenize(text)\n",
        "             if word.lower() not in stop_words]))"
      ],
      "metadata": {
        "id": "46baWDmOCkIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sent_dataset(filename):\n",
        "  # define an empty pandas dataframe\n",
        "  data_df = pd.DataFrame.from_dict({})\n",
        "\n",
        "  # for each category, create an empty list for storing all extracted entitites\n",
        "  header_list = {header:[] for header, _ in header_tag}\n",
        "\n",
        "  # store the sentence ids and sentences\n",
        "  id_sents = zip(*[(sentID,sent.strip()) for sentID, sent in\n",
        "            enumerate(sent_tokenize(cleaned_texts[filename]))])\n",
        "\n",
        "  data_df['sent_id'], data_df['sentence'] = list(id_sents)\n",
        "\n",
        "  # Extract and store all entity categories found in each sentence\n",
        "  pbar = tqdm(enumerate(data_df['sentence']))\n",
        "  for i, sent in pbar:\n",
        "    doc = nlp(sent)\n",
        "    for header, tag in header_tag: header_list[header].append(tagger(doc, tag))\n",
        "    pbar.set_description(f\"-{filename[:-4]} sent {i:003d}\")\n",
        "\n",
        "  for header, tag in header_tag: data_df[header]=header_list[header]\n",
        "\n",
        "  # include sentiment scores\n",
        "  data_df['sentiment_score']= (data_df['pos_words'].apply(len) - data_df['neg_words'\n",
        "                                        ].apply(len))/data_df['sentence'].apply(\n",
        "                                            lambda x : len(pre_process_text(x)))\n",
        "\n",
        "  return data_df"
      ],
      "metadata": {
        "id": "2znHrUro9ggv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating and exploring the datasets from files"
      ],
      "metadata": {
        "id": "XO7-auCy7IVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating all the datasets for all the corpus files. May take a while to complete..."
      ],
      "metadata": {
        "id": "hwleUnZzJ-4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tables = {f:generate_sent_dataset(f) for f in sorted(cleaned_texts.keys())}"
      ],
      "metadata": {
        "id": "jmBBTR35D-Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the top 5 rows of one of the files `'Anon_cqp_66.xml'`...\n"
      ],
      "metadata": {
        "id": "CcFBMNcQHqh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tables['Anon_cqp_66.xml'].head()"
      ],
      "metadata": {
        "id": "y0J2x5fGFLcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the sentiments on the sentences"
      ],
      "metadata": {
        "id": "BSDgIm21PcD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLos4pwZMHKA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"lightgreen\", \"green\"])\n",
        "normalize = lambda x: (x-np.min(x))/(np.max(x)-np.min(x))"
      ],
      "metadata": {
        "id": "nBoGT2OjOf4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_sentiments(filename):\n",
        "  data = data_tables[filename]\n",
        "  data['norm_sentiment'] = normalize(data['sentiment_score'])\n",
        "  data.plot.scatter('sent_id', 'norm_sentiment', c='norm_sentiment', colormap=cmap, figsize=(10, 5))"
      ],
      "metadata": {
        "id": "QUjmY24YOjI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_sentiments('Anon_cqp_66.xml')"
      ],
      "metadata": {
        "id": "fRtCCzk_JFDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### Select filenames to change the plot...{ run: \"auto\" }\n",
        "\n",
        "choose_filename = 'Anon_cqp_66.xml' #@param ['Anon_cqp_66.xml', 'Bree_cqp_56.xml', 'Carter_cqp_52.xml', 'Collingwood_cqp_75.xml', 'Denholm_cqp_35.xml', 'Gell_cqp_29.xml', 'Hawthorne_cqp_70.xml','Ostell_cqp_34.xml', 'Southey_cqp_40.xml', 'Walker_cqp_25.xml'] {allow-input: true}\n",
        "\n",
        "plot_sentiments(choose_filename)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Vrd13jEhN30i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further ideas for the data tables"
      ],
      "metadata": {
        "id": "Y99NIeCFV1PG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the data tables (or data frames), you can attempt write codes that can query, extract and visualise your data in different ways e.g.:\n",
        "\n",
        "*   Top placenames in a file\n",
        "*   Top geonouns in a file\n",
        "*   Top co-occurring geonouns to a place name\n",
        "*   Search for sentences with specific placenames or geonouns or any combination of both\n",
        "\n",
        "Also, the datasets form the basis for training a more complex machine learning models\n",
        "\n"
      ],
      "metadata": {
        "id": "vmdeN9XHWDI-"
      }
    }
  ]
}