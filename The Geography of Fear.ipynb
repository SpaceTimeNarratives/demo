{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["yqL4KZ0qutYJ","4qzsXW5dujG3","O3Q8jrPA9EYv","RzUcuqsLMAFi","A00lSJoQgvc2","JidIIYSfXz-r","ro_elIv_wfPp"],"mount_file_id":"1OGEyJ9ZvAGvlQKf3cLklOirCHeiFF2_P","authorship_tag":"ABX9TyNADuyy2xVJOy+acPTOVNlD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## CLARIN-EHRI Workshop, Prague March 27-28, 2024"],"metadata":{"id":"l5x62i5Et7kM"}},{"cell_type":"markdown","source":["### Introduction"],"metadata":{"id":"yqL4KZ0qutYJ"}},{"cell_type":"markdown","source":["**Workshop Title:**\n","*The Geography of **fear**: Exploring the Emotional\n","Landscapes in the Holocaust Survivorsâ€™ Testimonies*\n","\n","**Background:** Holocaust survivors' testimonies offer a profound insight into the individual experiences endured during the Nazi genocide. They reveal emotional connections to places, events, and memories, forming what is known as emotional geography. This field helps in understanding the interplay of emotions such as fear, anger, surprise, disgust, and joy across various locations and times. Extracting and analyzing these emotions from vast textual data collections is challenging, but this work aims to explore this possibility using natural language processing techniques.\n","\n"],"metadata":{"id":"0LyeS6tqlp-Z"}},{"cell_type":"markdown","source":["Let's start by getting into our working with the command:\n","\n","```python\n","cd clarin_ehri_prague_2024\n","```\n","\n","Uncomment (i.e. delete the `#` symbol) below and execute the command"],"metadata":{"id":"ABjzlgs_7LCV"}},{"cell_type":"code","source":["cd /content/drive/MyDrive/clarin_ehri_prague_2024"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FudH2A506muM","executionInfo":{"status":"ok","timestamp":1711613159823,"user_tz":0,"elapsed":411,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"462e589d-02f9-49eb-9757-a148d115a16c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/clarin_ehri_prague_2024\n"]}]},{"cell_type":"code","source":["# Install and import libraries\n","!pip -q install geonamescache # helps us acces the geonames list\n","!python -q -m spacy download en_core_web_trf # use the spacy transformer model\n","from geonamescache import GeonamesCache as gc\n","import os, shutil, re\n","from collections import defaultdict, OrderedDict, Counter\n","from IPython.display import HTML\n","import pandas as pd\n","import nltk\n","import spacy\n","# from spacy import displacy\n","from nltk import sent_tokenize\n","from transformers import pipeline\n","\n","nltk.download('punkt')"],"metadata":{"id":"1yM5Rij9e1Rt","executionInfo":{"status":"ok","timestamp":1711620061773,"user_tz":0,"elapsed":250,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["### Task 1: Read and process testimony file"],"metadata":{"id":"4qzsXW5dujG3"}},{"cell_type":"markdown","source":["We used data from the **[Holocaust Survivors' Testimonies](https://vha.usc.edu/home)** dataset. They contain the question-answer pairs from interviews granted by holocaust survivors as well as other annotations `emotion` (negative and positive words) with sentiment scores, `city`, `camp`, `geonoun`, `expression`, and `other_language`.\n","\n","For this workshop, we will annotate only 10 randomly testimonies each of:\n","- `268.txt`, `36999.txt`, `37210.txt`, `37250.txt`, `37409.txt`, `37556.txt`, `37567.txt`, `37585.txt`, `37605.txt`, `37648.txt`.\n","- They are contained in the `data` folder\n","\n","Uncomment below to use the `ls` command to list the files in the `data` folder:\n"],"metadata":{"id":"Pmwsd8-mtr-G"}},{"cell_type":"code","source":["# ls data"],"metadata":{"id":"imAR-hEJ8WmS","executionInfo":{"status":"ok","timestamp":1711613140426,"user_tz":0,"elapsed":7,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["#### But what does as VHA testimony file look like?\n","\n","Let's define a function `readfile` as below to take a testimony file name and read it into memory in a neat way removing the blank lines.\n","\n","```python\n","readfile = lambda fname: [line.strip() for line in open(f'data/{fname}').readlines() if line.strip()]\n","```"],"metadata":{"id":"O3Q8jrPA9EYv"}},{"cell_type":"code","source":["readfile = lambda fname: [line.strip() for line in open(f'data/{fname}').readlines() if line.strip()]\n","# readfile('268.txt')"],"metadata":{"id":"EkwnSXiv9Dqr","executionInfo":{"status":"ok","timestamp":1711619946565,"user_tz":0,"elapsed":6,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# @title **Exercise 1:** Can you write the command to read file `37210.txt`?\n","\n","# Delete me and write your code..."],"metadata":{"cellView":"form","id":"H1JgfFJ2FRJ9","executionInfo":{"status":"ok","timestamp":1711613208561,"user_tz":0,"elapsed":421,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Task 2: Splitting the interview `questions` and `answers`\n","We may also separate the interviews questions from the survivor's responses and present them in a data frame. Let's define the functions for splitting the `questions and `answers` from testimony files"],"metadata":{"id":"RzUcuqsLMAFi"}},{"cell_type":"markdown","source":["###### Defining the functions"],"metadata":{"id":"A00lSJoQgvc2"}},{"cell_type":"code","source":["def get_survivor_initial(filename):\n","  initials=[] #list for storing all possible initials\n","  testimony=readfile(filename)\n","  for line in testimony:\n","    #search through each line and append any pattern that looks like an initial\n","    m = re.search('\\w*:', line)\n","    if m: initials.append(m.group())\n","  #from the most common 2 pick the one that is not the interviewer\n","  return testimony, [initial for initial, _ in Counter(initials).most_common()[:2] if initial!='INT:'][0]\n","\n","def get_questions_and_answers(filename):\n","  testimony, initial = get_survivor_initial(filename) # returns the initial of the speaker\n","  # split the interviews based on the interviewers questions or promptings\n","  qas = ['INT: '+qa for qa in ' '.join(testimony[2:]).split('INT: ')]\n","  # return pairs of question/promptings and answers/responses from the survivors\n","  questions, answers = list(zip(*[(qa.split(initial)[0],initial+qa.split(initial)[1]) for qa in qas if len(qa.split(initial))==2]))\n","  return pd.DataFrame.from_dict({'fileID':[filename[:-4]]*len(questions), 'questions':questions, 'answers':answers})"],"metadata":{"id":"CcrkPmpfNEUZ","executionInfo":{"status":"ok","timestamp":1711620138345,"user_tz":0,"elapsed":221,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["##### Processing testimony files\n","Do not bother about the functions but if you are curious, you can look click on the `Show code` and have a peep.\n","\n","But we'll use one of the functions `get_questions_and_answers()` function to transform a testimony into a dataframe with columns for `questions` and `answers`.\n"],"metadata":{"id":"5CPGgmqDN7aI"}},{"cell_type":"code","source":["get_questions_and_answers('268.txt')"],"metadata":{"id":"FKYIQvH3N55r","executionInfo":{"status":"ok","timestamp":1711620334227,"user_tz":0,"elapsed":222,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["Now use the same function we used above to transform testimony `37250.txt` into a dataframe with questions and answers"],"metadata":{"id":"FI8_nHScQFfl"}},{"cell_type":"code","source":["# @title **Exercise 2:** Transform file `37250.txt` to a dataframe?\n","\n","# Delete me and write your code..."],"metadata":{"id":"p9hID3isQGFn","executionInfo":{"status":"ok","timestamp":1711613238212,"user_tz":0,"elapsed":255,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Task 3: Split `answers` into sentences"],"metadata":{"id":"JidIIYSfXz-r"}},{"cell_type":"markdown","source":["Some responses are just too long and span many sentences e.g.response index 83 has 24135 characters or 4687 tokens. To confirm this, you can run the code below\n","```python\n","index_token_size = {i:len(answer.split()) for i, answer in enumerate(testimony_268['answers'])}\n","max(index_token_size, key=index_token_size.get)\n","```\n","followed by\n","```python\n","print(f\"\"\"characters: {len(testimony_268['answers'][83])}\n","tokens: {len(testimony_268['answers'][83].split())}\"\"\")\n","```\n","It will therefore be better to segment the answers further into sentences for better processing."],"metadata":{"id":"X7DzaI85aEBA"}},{"cell_type":"code","source":["testimony_268_qas = get_questions_and_answers('268.txt')"],"metadata":{"id":"ozk-KVSHVnBh","executionInfo":{"status":"ok","timestamp":1711615733860,"user_tz":0,"elapsed":222,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["fileIds, answerIds, sentences = [],[],[]\n","for i in range(len(testimony_268_qas)):\n","  fileID, questions, answers = testimony_268_qas.iloc[i]\n","  sents = sent_tokenize(testimony_268_qas.answers[i][4:])\n","  fileIds.extend([fileID]*len(sents))\n","  answerIds.extend([i]*len(sents))\n","  sentences.extend(sents)\n","testimony_268_sents = pd.DataFrame.from_dict({'fileID':fileIds, 'answerID':answerIds, 'sentences':sentences})"],"metadata":{"id":"U3nLGCn2VxEN","executionInfo":{"status":"ok","timestamp":1711615764972,"user_tz":0,"elapsed":321,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["### Task 4: Identifying places and other entities in the testimony"],"metadata":{"id":"hS3lCGCfPU-V"}},{"cell_type":"markdown","source":["##### Let's start by importing the `en_core_web_trf` model and adding the GEONOUN patterns"],"metadata":{"id":"_NI6Uq8UvyUL"}},{"cell_type":"code","source":["# import and load the spacy web transformer model\n","import en_core_web_trf\n","nlp = en_core_web_trf.load()\n","nlp.add_pipe('merge_entities')\n","\n","# Add the `entity_ruler` to the pipeline before the NER module\n","ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n","\n","# add patterns for label `CITY`, COUNTRY, CONTINENT, GEONOUN\n","patterns =  [{\"label\": \"GEONOUN\", \"pattern\": noun} for noun in open('combined_geonouns.txt').read().strip().split('\\n')]\n","ruler.add_patterns(patterns)"],"metadata":{"id":"RIKJHzqAsMZY","executionInfo":{"status":"ok","timestamp":1711623249701,"user_tz":0,"elapsed":78876,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["Then we use the `PlaceNames` and `Annotator` classes below to define our placenames and other entities as well as perform annotations. You can view the code to seehow it works or just trust me and use it as it is ðŸ˜€\n"],"metadata":{"id":"WrWI0XDGyXa_"}},{"cell_type":"code","source":["sortbylen = lambda lst: sorted(set(lst), key=lambda v:len(v), reverse=True)\n","class PlaceNames:\n","    def __init__(self):\n","      self.resources_url= \"https://raw.githubusercontent.com/SpaceTimeNarratives/demo/main/resources/\"\n","      self.__download_resources()\n","      self.additional_cities = ['New York'] #cities not in GeoNames or Aliases\n","      self.additional_countries = ['America', 'the United States','Czechoslovakia'] #countries not in GeoNames or Aliases\n","      self.cities, self.city_names = self.__get_cities()\n","      self.us_states, self.us_state_names = self.__get_us_states()\n","      self.countries, self.country_names = self.__get_countries()\n","      self.continents, self.continent_names = self.__get_continents()\n","      self.camps = self.__get_camps()\n","      self.geonouns = self.__get_geonouns()\n","      self.ambiguous_cities = self.__get_ambiguous_cities()\n","      # self.sentiment_scores  = None\n","      # self.emotion_scores    = None\n","\n","    # city details and names\n","    def __get_cities(self):\n","      __cities = {i:{'geonameid':detail['geonameid'], 'name':detail['name'].replace(\"'\",'â€™'),\n","             'latitude':float(detail['latitude']), 'longitude':float(detail['longitude']),\n","             'countrycode':detail['countrycode']} for i, (_, detail) in enumerate(gc().get_cities().items())}\n","      __names = [city['name'] for _, city in __cities.items()]\n","      __names.extend(self.additional_cities)\n","      return __cities, sortbylen(__names)\n","\n","    # US states details and names\n","    def __get_us_states(self):\n","      __us_states = {i:{'geonameid':detail['geonameid'],'name':detail['name'].replace(\"'\",'â€™'),'code':detail['code']}\n","              for i, (_, detail) in enumerate(gc().get_us_states().items())}\n","      __names = sortbylen([us_state['name'] for _, us_state in __us_states.items()])\n","      return __us_states, __names\n","\n","    # country details and names\n","    def __get_countries(self):\n","      __countries = {i:{'geonameid':detail['geonameid'], 'iso': detail['iso'], 'name':detail['name'].replace(\"'\",'â€™'),\n","                'capital':detail['capital'].replace(\"'\",'â€™'), 'continentcode':detail['continentcode'], 'neighbours':detail['neighbours']}\n","                for i, (_, detail) in enumerate(gc().get_countries().items())}\n","      __names = [country['name'] for _, country in __countries.items()]\n","      __names.extend(self.additional_countries)\n","      return __countries, sortbylen(__names)\n","\n","    # continent details and names\n","    def __get_continents(self):\n","      __continents = {i:{'geonameid':detail['geonameId'], 'name':detail['name'].replace(\"'\",'â€™'), 'continentcode':detail['continentCode'],\n","                 'bbox_north':detail['bbox']['north'], 'bbox_south':detail['bbox']['south'], 'bbox_east':detail['bbox']['east'],\n","                 'bbox_west':detail['bbox']['west']}  for i, (_, detail) in enumerate(gc().get_continents().items())}\n","      __names = sortbylen([continent['name'] for _, continent in __continents.items()])\n","      return __continents, __names\n","\n","    # ---------Other resources------------\n","    # Download resource file()\n","    def __download_resources(self):\n","      for res in ['cleaned_holocaust_camps.txt','combined_geonouns.txt','ambiguous_cities.txt']:\n","        if not os.path.exists(res):\n","          os.system(f\"wget -q {self.resources_url}{res}\")\n","          print(f\"{res} successfully downloaded.\")\n","\n","    def __read_source_file(self, source_file):\n","      return open(source_file).read().strip().split('\\n')\n","\n","  # Concentration camps\n","    def __get_camps(self, srcfile=None):\n","      source_file = srcfile if srcfile else 'cleaned_holocaust_camps.txt'\n","      __camps = self.__read_source_file(source_file)\n","      if __camps: return sortbylen([name for name in __camps if name not in [country['name']\n","                                                for _, country in self.countries.items()]])\n","      else:\n","        print(f\"Error: Reading file '{source_file}'.\")\n","        return None\n","\n","  # Geographical feature names\n","    def __get_geonouns(self, srcfile=None):\n","      source_file = srcfile if srcfile else 'combined_geonouns.txt'\n","      __geonouns = self.__read_source_file(source_file)\n","      if __geonouns: return sortbylen(__geonouns)\n","      else:\n","        print(f\"Error: Reading file '{source_file}'.\")\n","        return None\n","\n","  # Get ambiguous cities\n","    def __get_ambiguous_cities(self, srcfile=None):\n","      source_file = srcfile if srcfile else 'ambiguous_cities.txt'\n","      __ambiguous_cities = self.__read_source_file(source_file)\n","      if __ambiguous_cities: return sortbylen(__ambiguous_cities)\n","      else:\n","        print(f\"Error: Reading file '{source_file}'.\")\n","        return None\n","\n","  # Check Ambiguous Cities\n","    isCityAmbiguous = lambda self, city: (True, [_city for _, _city in self.cities.items() if _city['name'].lower() == city.lower()]\n","                                          ) if city in self.ambiguous_cities else (False,f\"{city} is not ambiguous: {[_city for _, _city in self.cities.items() if _city['name'].lower() == city.lower()][0]}\")\n","class Annotator(PlaceNames):\n","    def __init__(self, **kwargs): #kwargs = ['text', 'model']\n","      super().__init__()\n","      # self.file           = kwargs['file'] if 'file' in kwargs else None ##FIX LATER\n","      self.text             = kwargs['text'] if 'text' in kwargs else None\n","      self.emotion_model    = kwargs['emotion_model'] if 'model' in kwargs else None\n","      self.sentiment_model  = kwargs['sentiment_model'] if 'model' in kwargs else None\n","      self.entity_tags      = ['CONTINENT', 'COUNTRY', 'US-STATE', 'CITY', 'CAMP',\n","                               'DATE','TIME','GEONOUN']\n","      self.entities = self.__get_entities(self.text) if self.text else None\n","      self.output_dir ='output'\n","      self.__BG_COLOR={'CITY':'#feca74','COUNTRY':'#f0b6de','CONTINENT':'#e4e7d2','US-STATE':'#feca74',\n","                       'CAMP':'#b3d6f2','GEONOUN': '#9cc9cc','DATE':'#c7f5a9', 'TIME':'#a9f5bc',\n","                       'PLACE':'#e4e7d2', 'EVENT':'#e0aedd'}\n","    # merging two entities\n","    def __merge_entities(self, first_ents, second_ents):\n","      return dict(OrderedDict(sorted({**second_ents, **first_ents}.items())))\n","\n","    # merging two entities\n","    def __join_near_similar_ents(self, ent_dict, tag):\n","      return {i:(ent[0]+' '+ent_dict[i+len(ent[0])+1][0], tag)\n","              for i, ent in ent_dict.items() if ent[1]==tag and i+len(ent[0])+1 in ent_dict}\n","\n","    # extract entities from text\n","    def __get_entities(self, text=None):\n","      if text: self.text = text\n","      if self.text: doc = nlp(self.text)\n","      else: return f\"Error: 'Annotator' has no text to process!\"\n","\n","      __ent_details = {token.idx:(self.text[token.idx:token.idx+len(token)],\n","         token.ent_type_, token.pos_) for token in doc if token.ent_type_ in\n","          ['FAC','GPE','LOC','DATE','TIME','EVENT','GEONOUN']}\n","\n","      # enforce only 'GEONOUNS' pos-tagged as 'NOUN'\n","      __ent_details= {i:detail for i, detail in __ent_details.items() if detail[:2]!='GEONOUN' or (detail[:2]=='GEONOUN' and detail[:3]=='NOUN')}\n","\n","      #join near similar ents e.g. \"concentration:GEONOUN\", \"camp:GEONOUN\" --> \"concentration camp:GEONOUN\"\n","      __ent_details= self.__merge_entities(self.__join_near_similar_ents(__ent_details, 'GEONOUN'), __ent_details)\n","\n","      return {i:self.__convert_place_entities(detail[:2]) for i, detail in __ent_details.items()}\n","\n","    def __convert_place_entities(self, place):\n","      name, tag = place\n","      if tag in ['FAC','GPE','LOC']:\n","        if name in self.continent_names: return name, 'CONTINENT'\n","        elif name in self.country_names: return name, 'COUNTRY'\n","        elif name in self.us_state_names: return name, 'US-STATE'\n","        elif name in self.city_names: return name, 'CITY'\n","        elif name in self.camps: return name, 'CAMP'\n","        else: return name, 'PLACE'\n","      return name, tag\n","\n","    def __get_tagged_list(self, text, __ent_details):\n","      entities = {i:self.__convert_place_entities(detail[:2]) for i, detail in __ent_details.items()}\n","      begin, tokens_tags = 0, []\n","      for start, (ent, tag) in entities.items():\n","        if begin <= start:\n","          tokens_tags.append((text[begin:start], None))\n","          tokens_tags.append((text[start:start+len(ent)], tag))\n","          begin = start+len(ent)\n","      tokens_tags.append((text[begin:], None)) #add the last untagged chunk\n","      return tokens_tags\n","\n","    def __mark_up(self, token, tag=None):\n","      if tag:\n","        begin_bkgr = f'<bgr class=\"entity\" style=\"background: {self.__BG_COLOR[tag]}; padding: 0.1em 0.1em; margin: 0 0.15em; border-radius: 0.23em;\">'\n","        end_bkgr = '\\n</bgr>'\n","        begin_span = '<span style=\"font-size: 0.8em; font-weight: bold; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">'\n","        end_span = '\\n</span>'\n","        return f\"{begin_bkgr}{token}{begin_span}{tag}{end_span}{end_bkgr}\"\n","      return f\"{token}\"\n","\n","    def visualize(self):\n","      token_tag_list = self.__get_tagged_list(self.text, self.entities)\n","      start_div = f'<div class=\"entities\" style=\"line-height: 2.0; direction: ltr\">'\n","      end_div = '\\n</div>'\n","      html = start_div\n","      for token, tag in token_tag_list:\n","        html += self.__mark_up(token,tag)\n","      html += end_div\n","      return HTML(html)"],"metadata":{"id":"KGswRU6cPS9C","executionInfo":{"status":"ok","timestamp":1711623320831,"user_tz":0,"elapsed":240,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["text = testimony_268_qas.answers[7]\n","text"],"metadata":{"id":"GHd02P65UCbH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["annotator = Annotator(text=text)"],"metadata":{"id":"X0Df12oQcRA-","executionInfo":{"status":"ok","timestamp":1711624564808,"user_tz":0,"elapsed":2213,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["#@title We can list the entities we have extracted...\n","annotator.entities"],"metadata":{"cellView":"form","id":"6Khrjz8ydhBA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title ...or even visualize them.\n","annotator.visualize()"],"metadata":{"cellView":"form","id":"CB6fB5pntXgE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["  ### Task 4: Emotion Classification"],"metadata":{"id":"LMvmUtaL1UFY"}},{"cell_type":"markdown","source":["For emotions we will use this transformer model here: [j-hartmann/emotion-english-distilroberta-base](j-hartmann/emotion-english-distilroberta-base)"],"metadata":{"id":"zpYcbGBv2XOD"}},{"cell_type":"code","source":["# testimony_268_sents"],"metadata":{"id":"7xmCXan80F2I","executionInfo":{"status":"ok","timestamp":1711625299946,"user_tz":0,"elapsed":236,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","# load pre-trained emotion classification model\n","model_path = \"j-hartmann/emotion-english-distilroberta-base\"\n","model = pipeline(\"text-classification\", model=model_path, tokenizer=model_path,\n","                        max_length=512, truncation=True)"],"metadata":{"id":"9JhhxctW1nnY","executionInfo":{"status":"ok","timestamp":1711625926927,"user_tz":0,"elapsed":935,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["for i in range(20):\n","  testimony = testimony_268_sents.sentences[i]\n","  score = model(testimony)\n","  print(f\"{testimony}\\n- {score[0]['label']}, {score[0]['score']}\")\n","  print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bGxnogPC0cfc","executionInfo":{"status":"ok","timestamp":1711626710509,"user_tz":0,"elapsed":7403,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"3a8e75a7-db2e-4e83-f90c-d2de7fa9a8af"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["My name is Henry Rosmarin.\n","- neutral, 0.9009859561920166\n","\n","Yes.\n","- neutral, 0.8983097672462463\n","\n","R-O-S-M-A-R-I-N.\n","- neutral, 0.7591530680656433\n","\n","At birth it was Henryk, H-E-N-R-Y-K Rozmaryn, R-O-S-M-A-R-Y-- I'm sorry, English spelling.\n","- sadness, 0.7981343269348145\n","\n","Polish spelling, R-O-Z-M-A-R-Y-N, Rozmaryn.\n","- neutral, 0.8772909641265869\n","\n","October 7th, 1925.\n","- neutral, 0.3719763457775116\n","\n","My present age is 73.\n","- sadness, 0.6599793434143066\n","\n","I was born in a little town called Czeladz, in Poland.\n","- joy, 0.9255752563476562\n","\n","It's southwestern corner of Poland.\n","- neutral, 0.9070968627929688\n","\n","I spell it for you, C-- capital C-Z-E-L-A-D-Z.\n","- neutral, 0.935821533203125\n","\n","The nearest town was Bedzin, Sosnowiec.\n","- neutral, 0.7137645483016968\n","\n","And towards the German border was Katowice and Siemianowice.\n","- neutral, 0.5386792421340942\n","\n","Incidentally, it was in Siemianowice that I lived.\n","- neutral, 0.7330958843231201\n","\n","Actually, I was an infant.\n","- fear, 0.4007508158683777\n","\n","My mom told me she went to Czeladz because my father-- my-- my fraternal grandparents lived in Czeladz.\n","- sadness, 0.36516067385673523\n","\n","And so this was before hospital births, you know.\n","- neutral, 0.9152296781539917\n","\n","This was birth by a midwife, of course.\n","- neutral, 0.525619387626648\n","\n","So she went to Czeladz, which was, like, three kilometers, and gave birth-- birth [INAUDIBLE] to yours truly.\n","- neutral, 0.6897490620613098\n","\n","And then she went back when I was probably two weeks old, back to Siemianowice, where I lived until-- for the next few years, of course.\n","- neutral, 0.7363588809967041\n","\n","My father's name was Abraham.\n","- neutral, 0.8323153853416443\n","\n"]}]},{"cell_type":"code","source":["# @title **Emotion 10**: Define `filenames` from the working directory\n","# emotion_10_zip_file = '/content/drive/MyDrive/UCREL/demo/resources/ht_resources/data/emotion_scores_10.zip'\n","!wget -c = 'https://github.com/SpaceTimeNarratives/demo/raw/main/resources/emotion_scores_10.zip'\n","shutil.unpack_archive(emotion_10_zip_file)\n","filenames = ['268', '36999', '37210', '37250', '37409', '37556', '37567', '37585', '37605', '37648']\n","file_paths = [f'emotion_scores/{f}_emotion_scores.xlsx' for f in filenames if os.path.exists(f'emotion_scores/{f}_emotion_scores.xlsx')]\n","os.listdir()"],"metadata":{"id":"g8OPFsAM6W_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!wget -c = 'https://github.com/SpaceTimeNarratives/demo/raw/main/resources/emotion_scores_10.zip'"],"metadata":{"id":"lWBroVhC7bzH","executionInfo":{"status":"ok","timestamp":1711627167285,"user_tz":0,"elapsed":1069,"user":{"displayName":"Ignatius Ezeani","userId":"00944355542181313663"}},"outputId":"7f9bf09c-4559-4331-aed9-a6f6f7a5de8c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-28 11:59:24--  http://=/\n","Resolving = (=)... failed: Name or service not known.\n","wget: unable to resolve host address â€˜=â€™\n","--2024-03-28 11:59:24--  https://github.com/SpaceTimeNarratives/demo/raw/main/resources/emotion_scores_10.zip\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/SpaceTimeNarratives/demo/main/resources/emotion_scores_10.zip [following]\n","--2024-03-28 11:59:24--  https://raw.githubusercontent.com/SpaceTimeNarratives/demo/main/resources/emotion_scores_10.zip\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 416 Range Not Satisfiable\n","\n","    The file is already fully retrieved; nothing to do.\n","\n"]}]}]}